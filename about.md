---
layout: page_photo_header
title: About MIRAI
<<<<<<< HEAD
image_class: headerimage
=======
<<<<<<< HEAD
image_class: headerimage
---
<br>  
=======
image_class: headerimage_people
>>>>>>> bcde8da5a92d75d0e2e295607065ae2dfbb8c31e
lang: en
lang-ref: about
---
>>>>>>> 806835bf6b4280b9b106e2da4a3f294abb27eee6

## I. State of Knowledge

<br>  

For centuries, libraries have been the guardians of human knowledge. These venerable institutions collected information in print and manuscript formats and made it accessible by providing tools (e.g., card catalogues) for searching and locating information. In the past few decades, the function of these institutions has changed dramatically. The Internet has become the primary source of information. With the aid of web-based search engines, such as Google, anyone with an Internet connection can easily locate and search large amounts of textual information. Text-based information retrieval has permeated every aspect of our lives.

A comparable shift in the accessibility of musical information has yet to take place. Scores and audio
remain the primary means of transmitting musical information. However, neither of these forms is directly searchable. Scores can be searched once they have been converted to a computer-readable format, but the technology and resources required for the conversion are not readily available. Even if scores were widely available in a computer-readable format, we lack the tools to search them. The search problem is even more complex for audio. Despite over a decade of research in music information retrieval, the question of how to search audio files remains a difficult and, as of yet, only partially solved problem.

Although the computer-aided study of music information has a long history, few projects have resulted in tools and practices that can be integrated horizontally across different research programs. Until recently, little music was available in computer-searchable formats and the technology for converting images of musical scores into such a format (Optical Musical Recognition [OMR]) was not reliable enough. Additionally, the computer-aided study of music information has lacked a flagship research program, central objectives, and a view of the “bigger picture.”

<br>  

## II. Innovation and Complementarity in the Research Axes

<br>

There are two main research axes in this program: **Content** and **Searching & Analysis**. The Content axis collects and organizes music-related documents in various formats (e.g., scores, audio, and biographical information about composers) so that users can find what they are looking for easily and efficiently. It encompasses projects concerning the improvement of OMR technology (Burgoyne et al. 2009, 2008, 2007a; Charalampos et al. 2014; Fujinaga 2004; Hankinson 2014; Hankinson et al. 2012a, 2010), metadata management (Angeles et al. 2010), biographical data acquisition (prosopography) (Fujinaga 2014, Weiss et al. 2010), and audio feature extraction (McKay et al. 2006a; McEnnis et al. 2006a). Until now, the only OMR technology available was proprietary “black box” software that cannot be improved or adjusted for different types of notation. Our OMR software is open-source and can be trained to work on different types of notation.

The Searching & Analysis axis creates tools and techniques for searching and analyzing the information collected from the Content axis. From audio sources, we are creating tools for automatic genre classification (McKay et al. 2010b, 2009a, 2009b, 2006c, 2005a, 2005b, 2005c, 2004), harmonic analysis (Burgoyne et al. 2013, 2011, 2007b), performance analysis (Devaney et al. 2011a, 2011b, 2011c), and timbre recognition (Fujinaga et al. 2000). From scores in symbolic notation we are developing tools for searching and analyzing melody, harmony, counterpoint, and structure (Cumming et al. 2015a; Fujinaga et al. 2013). From text documents about music we are developing ways to link data that will reveal new connections among musicians, composers, patrons, places, and institutions.

Both axes examine how users interact with digital music and continually improve the tools we have developed (Hankinson et al. 2012b, 2012c, 2009, Burlet et al. 2012). Figure 1 shows the overall structure with examples of research projects under each axis.

![]({{ site.url }}{{ site.baseurl }}/assets/img/Structure_of_MIRAI.png)

<br>  

The two research axes are independent but complementary. Content that is not searchable in intelligent ways is not useful. Search and analysis algorithms require content to work on. Work can proceed on the two axes independently, but ultimately it is the combination that makes our research program valuable. The “User Interface” box in Figure 1 points to a feature of our program that applies to both axes: tools that are easy to use have the potential for broad impact. With our complementary research axes, the MIRAI team is building efficient and user-friendly tools to accommodate large-scale distributed processing. With these tools, libraries and archives anywhere in the world will be able to process their music collections and make them available to wider audiences for study, performance, and appreciation.

<br>  

<<<<<<< HEAD
## III. The SIMSSA Team

<br>  

We are a [diverse team](https://simssa.ca/people) dedicated to accomplishing this mission. SIMSSA includes music scholars, performers, librarians, and music technologists, working to create new tools for research and analysis of the collections of our partner museums, research libraries, and universities. Document processing and OMR correction for this collection will be carried out by musicians, students, and scholars around the world.



<br>  

<br>  


<hr>  

<br>  

## I. État des Connaissances

<br>  

Pendant des siècles, les bibliothèques ont été les gardiennes du savoir humain. Ces institutions vénérables ont longtemps collecté des documents sous forme imprimées ou manuscrites, et ont permis l’accès à leurs informations grâce à des outils adaptés à leur exploration et à leur localisation tels, par exemple, que les cartes de catalogue. Au cours des dernières décennies, la fonction de ces institutions a drastiquement changé, Internet étant devenu le principal pourvoyeur d’information. En effet, à l’aide de moteurs de recherche tels que Google, toute personne équipée d’une connexion à Internet peut aisément localiser et explorer de grandes quantités d’informations textuelles au point que ce type de recherche est présent dans tous les aspects de notre vie.

Cependant, un tel changement ne s’est pas encore opéré pour les informations musicales. Même si les partitions et les documents audio restent les principaux moyens de transmission, aucun de ces formats ne peut être directement exploré. La technologie et les ressources nécessaires à la conversion de partitions en un format lisible par l’ordinateur sont difficiles à se procurer, et même lorsque cela est possible, les outils de recherches n’existent pas. Le problème est encore plus complexe lorsque le document est sous format audio. Malgré plus de dix ans de recherches, l’exploration de ces documents reste un problème difficile à résoudre.

Bien que l’étude de la musique à l’aide des ordinateurs ait une longue histoire, peu de projets ont débouché sur des outils ou des pratiques intégrables horizontalement à travers différents programmes de recherche. Jusque récemment, les documents directement disponibles en un format permettant l’exploration par ordinateur étaient très rares. De plus, la technologie pour convertir l’image d’une partition en un tel format n’était pas fiable (Reconnaissance Optique de la Musique [ROM]). Enfin, les études portant sur l’information musicale ont manqué jusque là d’un pôle de recherche, d’objectifs précis et d’une vue d’ensemble.

<br>  

## II. Originalité et Complémentarité des Axes de Recherches

<br>  

Il y a deux principaux axes de recherche dans ce programme: **Contenu** et **Recherche et Analyse**. Le premier axe (**Contenu**) collecte et organise les documents musicaux en divers formats que les utilisateurs peuvent parcourir facilement et efficacement (partitions, audio, informations biographiques, etc.). Il couvre les projets portant sur l’amélioration de la technologie ROM (Burgoyne et al. 2009, 2008, 2007a; Charalampos et al. 2014; Fujinaga 2004; Hankinson 2014; Hankinson et al. 2012a, 2010), sur la gestion des métadonnées (Angeles et al. 2010), sur l’acquisition des données biographiques, ou prosopographie (Fujinaga 2014, Weiss et al. 2010), et l’extraction de données audio (McKay et al. 2006a; McEnnis et al. 2006a). Jusqu’à présent, la seule technologie ROM disponible était un logiciel-propriétaire en “boite noire” qu’il n’était possible ni d’améliorer ni d’adapter à différent types de notation. Notre logiciel OMR est libre d’accès et peut être entraîné pour de telles situations.

Le second axe (**Recherche et Analyse**) porte sur le développement d’outils et de techniques destinés à la recherche et à l’analyse des informations réunies par le premier axe (**Contenu**). À partir de documents audio, nous créons des outils de classification automatique de genres musicaux (McKay et al. 2010b, 2009a, 2009b, 2006c, 2005a, 2005b, 2005c, 2004), d’analyse harmonique (Burgoyne et al. 2013, 2011, 2007b), d’analyse de l’interprétation (Devaney et al. 2011a, 2011b, 2011c), et de reconnaissance des timbres instrumentaux (Fujinaga et al. 2000). À partir de partitions en notation symbolique, nous développons des outils capables d’explorer et d’analyser la mélodie, l’harmonie, le contrepoint et la structure de la musique (Cumming et al. 2015a; Fujinaga et al. 2013). Enfin, à partir des documents textuels sur la musique, nous relions entre elles des données qui éclairent d’un jour nouveau les interactions entre musiciens, compositeurs, mécènes, lieux de concerts et institutions.

Ces deux axes examinent la façon dont les utilisateurs interagissent avec la musique digitale et améliorent continuellement les outils développés jusqu’ici (Hankinson et al. 2012b, 2012c, 2009, Burlet et al. 2012). La Figure 1 en illustre la structure globale avec divers exemples de projets de recherche. Ils sont indépendants mais complémentaires: un contenu que l’on ne peut explorer de manière intelligente est inutile ; et les algorithmes de recherche et d’analyse ne fonctionnent que s’ils ont un contenu bien ordonné sur lequel s’exécuter. Il est possible de travailler indépendamment sur chacun d’entre eux, mais c’est leur combinaison qui donne à notre programme de recherche toute sa valeur. La boîte “Interface utilisateur” de la Figure 1 en est un exemple : les outils faciles d’utilisation ont une portée plus large.

Grâce à la complémentarité de nos deux axes de recherche, l’équipe de MIRI met au point des outils efficaces et développés pour l’utilisateur, qui facilitent les recherches de grande envergure. Ils permettront aux bibliothèques et archives du monde entier de traiter leurs collections musicales et d’en faciliter l’accès au public désireux de les étudier, de les jouer et de les apprécier.

<br>  

<br>  


<!-- <br>  

<br>  


## Intensity of Scientific Activity: Highlights of Research Achievements During the FRQSC Emerging Team Grant (2014–2015)

<br>  

### Cantus Ultimus (Content Axis)

<br>  

The Cantus Ultimus project applies the latest OMR technologies to plainchant manuscripts in order to
transform the existing CANTUS database (directed by **Lacoste**) of nearly 400,000 chant records into a state-of-the-art research environment in which both music and text are fully searchable. Within the past year, we have processed and made publically available online two of the oldest surviving chant manuscripts, the late 10th-c. St. Gall manuscripts (CH-SGs 390 and 391), and completed the OMR of the Salzinnes antiphonal (cantus.simssa.ca/manuscripts/).

<br>  

### New Version of Diva (Content Axis)

<br>  

Diva (ddmal.github.io/diva.js/) is a web-based, open-source digital document viewer, developed by **Hankinson** (postdoctoral researcher), Wendy Liu, and Evan Magoni, and managed by **Fujinaga** and **Pugin**. Diva was designed for websites of libraries, archives, and museums so that they could present high-resolution images of documents in a user-friendly interface optimized for speed and flexibility. The new version, released in August 2015, supports the International Image Interoperability Framework (IIIF). The IIIF is an important new initiative committed to developing a set of common interfaces that support interoperability between image repositories, facilitating horizontal integration across libraries and archives all over the world.

<br>  

### Improved ELVIS Database (Content Axis)

<br>  

The ELVIS Database (elvisproject.ca/) is an open, crowd-sourced database of music in symbolic notation, maintained by MIRAI. This summer, undergraduate student Alex Parmentier improved the search capabilities of the database, making it more powerful and adaptable, and added new functions allowing users to upload new pieces and modify pieces they have uploaded. In collaboration with our project manager and graduate students in musicology, he clarified guidelines for data entry and made the interface more attractive and user friendly. Our continued building of the database (now totalling over 6,000 movements and pieces) has made large-scale corpus studies possible.

<br>  

### Increased Flexibility of the VIS Counterpoint Web Application (Searching & Analysis Axis)

<br>  

Our open-source software for analyzing counterpoint, VIS, and its corresponding web application (counterpoint.elvisproject.ca/) make large-scale corpus studies accessible to music theorists and musicologists without programming experience. Within the last year, we have made significant improvements to the stability and flexibility of VIS. In summer 2015, Ryan Bannon (undergraduate student and lead programmer on VIS) began integrating VIS into Rodan, a workflow engine developed by **Hankinson**. This integration involved the low-level code integration of VIS tasks into Rodan such that they can be realized as individual atomic workflow tasks, and the development of a user-friendly web application that allows researchers to generate VIS-based workflows via a graphical user interface. We are on schedule to release our web application in spring 2016.

<br>  

### New Version of jSymbolic (Searching & Analysis Axis)

<br>  

jSymbolic is a software tool developed by **McKay** for analyzing symbolic music files by extracting a range of characteristic statistical information (called “features”) relating to musical elements such as pitch, rhythm, harmony, instrumentation, dynamics, and texture. In 2015, **McKay** and Tristano Tenaglia (undergraduate student) developed a new version that can extract features from MEI files (Music Encoding Initiative: an open-source, computer-readable music encoding format) and from windowed sections of a score (rather than the score in its entirety). Additionally, jSymbolic is now able to export features into WEKA ARFF, which will be useful for machine learning, facilitating such tasks as composer, style, or genre recognition.

<br>  

### jMei2Midi (Searching & Analysis Axis)

<br>  

**McKay** has also developed a new Java application and library called jMei2Midi, which can convert MEI files to the widely recognized MIDI file format. This allows us to process music accessible only in MEI with software not yet able to read MEI files. jMei2Midi’s general parsing libraries will also provide a useful resource for developers in the process of incorporating MEI-parsing capabilities into their own software.

<br>  

## Intensity of Scientific Activity: Publications During the FRQSC Emerging Team Grant

<br>  

### Context Axis

<br>  

*Optical Music Recognition (OMR):* Charalampos, **Hankinson**, & **Fujinaga** 2014; **Helsen**, **Bain**, **Fujinaga**, **Hankinson**, & **Lacoste** 2014

*Digital Libraries:* **Cumming** 2014; **Fujinaga**, **Hankinson**, & **Cumming** 2014a; **Laplante**, **Hankinson**, **Cumming**, & **Fujinaga** 2015; **Pugin**, Zitellini, & Roland 2014; Roland, **Hankinson, & **Pugin** 2014

<br>  

### Searching and Analysis Axis

<br>  

*Analysis of Symbolic Scores:* Antila & **Cumming** 2014; **Cumming** & **Schubert** 2015a; Risk, Mok, **Hankinson**, & **Cumming** forthcoming; **Schubert** & **Cumming** 2015; Sigler, **Wild**, & Handelman forthcoming; Winters & **Cumming** 2014

*Corpus Study:* **Cumming** & **Schubert** 2015b

*Music Perception & Big Data:* **Fujinaga**, Sears, & **Hankinson** 2014b; Goebl, Bresin, & **Fujinaga** 2014; Siedenburg, Fujinaga, & McAdams 2014; Vigliensoni & Fujinaga 2014

*Prosopography:* **Fujinaga** 2014

*User Interface/User Experience:* **Bain**, Behrendt, & **Helsen** 2014

<br>  

## Intensity of Scientific Activity: Future Research Plans

<br>  

### Development of a Single Interface for Searching & Analysis

<br>  

As we continue to develop OMR technologies and grow our database of scores in symbolic notation (ELVIS), we will improve methods of searching and analyzing this data. This will involve harvesting metadata from our partner institutions (for details see p. 10, under SSHRC Partnership Grant) and linking it with the OMR data, allowing us, for example, to search for a particular melody in works from 1400–50 with Latin texts. Our goal is the equivalent of Google Books for music in both audio and symbolic formats.

In addition to the counterpoint web application, our postdoctoral researcher **Krämer** is working on a melody search tool that includes pattern recognition engines based on combinatorial mathematics; an automatic mode detector; state transition matrix generators for analyzing melody, harmony, and counterpoint; and visualization tools of state transition networks. Doctoral students, such as Alexander Morgan, have developed tools for identifying and indexing a variety of musical events ranging from simple notes and durations to complex chordal, contrapuntal, and syntactic patterns. **Rusch** and Bannon are developing tools for identifying cadential voice-leading strands. Although Rusch and Bannon’s tools are modelled on a corpus of Bach chorales, they will soon be applied to other repertoire.

What should a search tool look like for music? Should it be a keyboard, a textbox in which to input note letter names, or a music editor? As we address questions surrounding user interface and user experience, **Laplante’s** and **Chiasson-Taylor’s** expertise in the information behaviour of music researchers, librarians, and performers will be invaluable.

<br>  

### Large-Scale Corpus Research
=======
## III. The MIRAI Team
>>>>>>> 806835bf6b4280b9b106e2da4a3f294abb27eee6

<br>  

We are a [diverse team](https://miraiqc.ca/people/) dedicated to accomplishing this mission. MIRAI includes music scholars, performers, librarians, and music technologists, working to create new tools for research and analysis of the collections of our partner museums, research libraries, and universities. Document processing and OMR correction for this collection will be carried out by musicians, students, and scholars around the world.



<br>  

<br>  


<hr>  

<br>  

<br>  


## I. État des Connaissances

<br>  

Pendant des siècles, les bibliothèques ont été les gardiennes du savoir humain. Ces institutions vénérables ont longtemps collecté des documents sous forme imprimées ou manuscrites, et ont permis l’accès à leurs informations grâce à des outils adaptés à leur exploration et à leur localisation tels, par exemple, que les cartes de catalogue. Au cours des dernières décennies, la fonction de ces institutions a drastiquement changé, Internet étant devenu le principal pourvoyeur d’information. En effet, à l’aide de moteurs de recherche tels que Google, toute personne équipée d’une connexion à Internet peut aisément localiser et explorer de grandes quantités d’informations textuelles au point que ce type de recherche est présent dans tous les aspects de notre vie.

Cependant, un tel changement ne s’est pas encore opéré pour les informations musicales. Même si les partitions et les documents audio restent les principaux moyens de transmission, aucun de ces formats ne peut être directement exploré. La technologie et les ressources nécessaires à la conversion de partitions en un format lisible par l’ordinateur sont difficiles à se procurer, et même lorsque cela est possible, les outils de recherches n’existent pas. Le problème est encore plus complexe lorsque le document est sous format audio. Malgré plus de dix ans de recherches, l’exploration de ces documents reste un problème difficile à résoudre.

Bien que l’étude de la musique à l’aide des ordinateurs ait une longue histoire, peu de projets ont débouché sur des outils ou des pratiques intégrables horizontalement à travers différents programmes de recherche. Jusque récemment, les documents directement disponibles en un format permettant l’exploration par ordinateur étaient très rares. De plus, la technologie pour convertir l’image d’une partition en un tel format n’était pas fiable (Reconnaissance Optique de la Musique [ROM]). Enfin, les études portant sur l’information musicale ont manqué jusque là d’un pôle de recherche, d’objectifs précis et d’une vue d’ensemble.

<br>  

## II. Originalité et Complémentarité des Axes de Recherches

<br>  

Il y a deux principaux axes de recherche dans ce programme: **Contenu** et **Recherche et Analyse**. Le premier axe (**Contenu**) collecte et organise les documents musicaux en divers formats que les utilisateurs peuvent parcourir facilement et efficacement (partitions, audio, informations biographiques, etc.). Il couvre les projets portant sur l’amélioration de la technologie ROM (Burgoyne et al. 2009, 2008, 2007a; Charalampos et al. 2014; Fujinaga 2004; Hankinson 2014; Hankinson et al. 2012a, 2010), sur la gestion des métadonnées (Angeles et al. 2010), sur l’acquisition des données biographiques, ou prosopographie (Fujinaga 2014, Weiss et al. 2010), et l’extraction de données audio (McKay et al. 2006a; McEnnis et al. 2006a). Jusqu’à présent, la seule technologie ROM disponible était un logiciel-propriétaire en “boite noire” qu’il n’était possible ni d’améliorer ni d’adapter à différent types de notation. Notre logiciel OMR est libre d’accès et peut être entraîné pour de telles situations.

Le second axe (**Recherche et Analyse**) porte sur le développement d’outils et de techniques destinés à la recherche et à l’analyse des informations réunies par le premier axe (**Contenu**). À partir de documents audio, nous créons des outils de classification automatique de genres musicaux (McKay et al. 2010b, 2009a, 2009b, 2006c, 2005a, 2005b, 2005c, 2004), d’analyse harmonique (Burgoyne et al. 2013, 2011, 2007b), d’analyse de l’interprétation (Devaney et al. 2011a, 2011b, 2011c), et de reconnaissance des timbres instrumentaux (Fujinaga et al. 2000). À partir de partitions en notation symbolique, nous développons des outils capables d’explorer et d’analyser la mélodie, l’harmonie, le contrepoint et la structure de la musique (Cumming et al. 2015a; Fujinaga et al. 2013). Enfin, à partir des documents textuels sur la musique, nous relions entre elles des données qui éclairent d’un jour nouveau les interactions entre musiciens, compositeurs, mécènes, lieux de concerts et institutions.

Ces deux axes examinent la façon dont les utilisateurs interagissent avec la musique digitale et améliorent continuellement les outils développés jusqu’ici (Hankinson et al. 2012b, 2012c, 2009, Burlet et al. 2012). La Figure 1 en illustre la structure globale avec divers exemples de projets de recherche. Ils sont indépendants mais complémentaires: un contenu que l’on ne peut explorer de manière intelligente est inutile ; et les algorithmes de recherche et d’analyse ne fonctionnent que s’ils ont un contenu bien ordonné sur lequel s’exécuter. Il est possible de travailler indépendamment sur chacun d’entre eux, mais c’est leur combinaison qui donne à notre programme de recherche toute sa valeur. La boîte “Interface utilisateur” de la Figure 1 en est un exemple : les outils faciles d’utilisation ont une portée plus large.

Grâce à la complémentarité de nos deux axes de recherche, l’équipe de MIRI met au point des outils efficaces et développés pour l’utilisateur, qui facilitent les recherches de grande envergure. Ils permettront aux bibliothèques et archives du monde entier de traiter leurs collections musicales et d’en faciliter l’accès au public désireux de les étudier, de les jouer et de les apprécier.

<br>  

<br>  
