---
layout: page_photo_header
title: About MIRAI
image_class: headerimage
---
<br>  

## I. State of Knowledge

<br>  

For centuries, libraries have been the guardians of human knowledge. These venerable institutions collected information in print and manuscript formats and made it accessible by providing tools (e.g., card catalogues) for searching and locating information. In the past few decades, the function of these institutions has changed dramatically. The Internet has become the primary source of information. With the aid of web-based search engines, such as Google, anyone with an Internet connection can easily locate and search large amounts of textual information. Text-based information retrieval has permeated every aspect of our lives.

A comparable shift in the accessibility of musical information has yet to take place. Scores and audio
remain the primary means of transmitting musical information. However, neither of these forms is directly searchable. Scores can be searched once they have been converted to a computer-readable format, but the technology and resources required for the conversion are not readily available. Even if scores were widely available in a computer-readable format, we lack the tools to search them. The search problem is even more complex for audio. Despite over a decade of research in music information retrieval, the question of how to search audio files remains a difficult and, as of yet, only partially solved problem.

Although the computer-aided study of music information has a long history, few projects have resulted in tools and practices that can be integrated horizontally across different research programs. Until recently, little music was available in computer-searchable formats and the technology for converting images of musical scores into such a format (Optical Musical Recognition [OMR]) was not reliable enough. Additionally, the computer-aided study of music information has lacked a flagship research program, central objectives, and a view of the “bigger picture.”

<br>  

## II. Innovation and Complementarity in the Research Axes

<br>

There are two main research axes in this program: **Content** and **Searching & Analysis**. The Content axis collects and organizes music-related documents in various formats (e.g., scores, audio, and biographical information about composers) so that users can find what they are looking for easily and efficiently. It encompasses projects concerning the improvement of OMR technology (Burgoyne et al. 2009, 2008, 2007a; Charalampos et al. 2014; Fujinaga 2004; Hankinson 2014; Hankinson et al. 2012a, 2010), metadata management (Angeles et al. 2010), biographical data acquisition (prosopography) (Fujinaga 2014, Weiss et al. 2010), and audio feature extraction (McKay et al. 2006a; McEnnis et al. 2006a). Until now, the only OMR technology available was proprietary “black box” software that cannot be improved or adjusted for different types of notation. Our OMR software is open-source and can be trained to work on different types of notation.

The Searching & Analysis axis creates tools and techniques for searching and analyzing the information collected from the Content axis. From audio sources, we are creating tools for automatic genre classification (McKay et al. 2010b, 2009a, 2009b, 2006c, 2005a, 2005b, 2005c, 2004), harmonic analysis (Burgoyne et al. 2013, 2011, 2007b), performance analysis (Devaney et al. 2011a, 2011b, 2011c), and timbre recognition (Fujinaga et al. 2000). From scores in symbolic notation we are developing tools for searching and analyzing melody, harmony, counterpoint, and structure (Cumming et al. 2015a; Fujinaga et al. 2013). From text documents about music we are developing ways to link data that will reveal new connections among musicians, composers, patrons, places, and institutions.

Both axes examine how users interact with digital music and continually improve the tools we have developed (Hankinson et al. 2012b, 2012c, 2009, Burlet et al. 2012). Figure 1 shows the overall structure with examples of research projects under each axis.

![]({{ site.baseurl }}/assets/img/Structure_of_MIRAI.png)

<br>  

The two research axes are independent but complementary. Content that is not searchable in intelligent ways is not useful. Search and analysis algorithms require content to work on. Work can proceed on the two axes independently, but ultimately it is the combination that makes our research program valuable. The “User Interface” box in Figure 1 points to a feature of our program that applies to both axes: tools that are easy to use have the potential for broad impact. With our complementary research axes, the MIRAI team is building efficient and user-friendly tools to accommodate large-scale distributed processing. With these tools, libraries and archives anywhere in the world will be able to process their music collections and make them available to wider audiences for study, performance, and appreciation.

<br>  

## III. The SIMSSA Team

<br>  

We are a [diverse team](https://simssa.ca/people) dedicated to accomplishing this mission. SIMSSA includes music scholars, performers, librarians, and music technologists, working to create new tools for research and analysis of the collections of our partner museums, research libraries, and universities. Document processing and OMR correction for this collection will be carried out by musicians, students, and scholars around the world.



<br>  

<br>  


<hr>  

<br>  

## I. État des Connaissances

<br>  

Pendant des siècles, les bibliothèques ont été les gardiennes du savoir humain. Ces institutions vénérables ont longtemps collecté des documents sous forme imprimées ou manuscrites, et ont permis l’accès à leurs informations grâce à des outils adaptés à leur exploration et à leur localisation tels, par exemple, que les cartes de catalogue. Au cours des dernières décennies, la fonction de ces institutions a drastiquement changé, Internet étant devenu le principal pourvoyeur d’information. En effet, à l’aide de moteurs de recherche tels que Google, toute personne équipée d’une connexion à Internet peut aisément localiser et explorer de grandes quantités d’informations textuelles au point que ce type de recherche est présent dans tous les aspects de notre vie.

Cependant, un tel changement ne s’est pas encore opéré pour les informations musicales. Même si les partitions et les documents audio restent les principaux moyens de transmission, aucun de ces formats ne peut être directement exploré. La technologie et les ressources nécessaires à la conversion de partitions en un format lisible par l’ordinateur sont difficiles à se procurer, et même lorsque cela est possible, les outils de recherches n’existent pas. Le problème est encore plus complexe lorsque le document est sous format audio. Malgré plus de dix ans de recherches, l’exploration de ces documents reste un problème difficile à résoudre.

Bien que l’étude de la musique à l’aide des ordinateurs ait une longue histoire, peu de projets ont débouché sur des outils ou des pratiques intégrables horizontalement à travers différents programmes de recherche. Jusque récemment, les documents directement disponibles en un format permettant l’exploration par ordinateur étaient très rares. De plus, la technologie pour convertir l’image d’une partition en un tel format n’était pas fiable (Reconnaissance Optique de la Musique [ROM]). Enfin, les études portant sur l’information musicale ont manqué jusque là d’un pôle de recherche, d’objectifs précis et d’une vue d’ensemble.

<br>  

## II. Originalité et Complémentarité des Axes de Recherches

<br>  

Il y a deux principaux axes de recherche dans ce programme: **Contenu** et **Recherche et Analyse**. Le premier axe (**Contenu**) collecte et organise les documents musicaux en divers formats que les utilisateurs peuvent parcourir facilement et efficacement (partitions, audio, informations biographiques, etc.). Il couvre les projets portant sur l’amélioration de la technologie ROM (Burgoyne et al. 2009, 2008, 2007a; Charalampos et al. 2014; Fujinaga 2004; Hankinson 2014; Hankinson et al. 2012a, 2010), sur la gestion des métadonnées (Angeles et al. 2010), sur l’acquisition des données biographiques, ou prosopographie (Fujinaga 2014, Weiss et al. 2010), et l’extraction de données audio (McKay et al. 2006a; McEnnis et al. 2006a). Jusqu’à présent, la seule technologie ROM disponible était un logiciel-propriétaire en “boite noire” qu’il n’était possible ni d’améliorer ni d’adapter à différent types de notation. Notre logiciel OMR est libre d’accès et peut être entraîné pour de telles situations.

Le second axe (**Recherche et Analyse**) porte sur le développement d’outils et de techniques destinés à la recherche et à l’analyse des informations réunies par le premier axe (**Contenu**). À partir de documents audio, nous créons des outils de classification automatique de genres musicaux (McKay et al. 2010b, 2009a, 2009b, 2006c, 2005a, 2005b, 2005c, 2004), d’analyse harmonique (Burgoyne et al. 2013, 2011, 2007b), d’analyse de l’interprétation (Devaney et al. 2011a, 2011b, 2011c), et de reconnaissance des timbres instrumentaux (Fujinaga et al. 2000). À partir de partitions en notation symbolique, nous développons des outils capables d’explorer et d’analyser la mélodie, l’harmonie, le contrepoint et la structure de la musique (Cumming et al. 2015a; Fujinaga et al. 2013). Enfin, à partir des documents textuels sur la musique, nous relions entre elles des données qui éclairent d’un jour nouveau les interactions entre musiciens, compositeurs, mécènes, lieux de concerts et institutions.

Ces deux axes examinent la façon dont les utilisateurs interagissent avec la musique digitale et améliorent continuellement les outils développés jusqu’ici (Hankinson et al. 2012b, 2012c, 2009, Burlet et al. 2012). La Figure 1 en illustre la structure globale avec divers exemples de projets de recherche. Ils sont indépendants mais complémentaires: un contenu que l’on ne peut explorer de manière intelligente est inutile ; et les algorithmes de recherche et d’analyse ne fonctionnent que s’ils ont un contenu bien ordonné sur lequel s’exécuter. Il est possible de travailler indépendamment sur chacun d’entre eux, mais c’est leur combinaison qui donne à notre programme de recherche toute sa valeur. La boîte “Interface utilisateur” de la Figure 1 en est un exemple : les outils faciles d’utilisation ont une portée plus large.

Grâce à la complémentarité de nos deux axes de recherche, l’équipe de MIRI met au point des outils efficaces et développés pour l’utilisateur, qui facilitent les recherches de grande envergure. Ils permettront aux bibliothèques et archives du monde entier de traiter leurs collections musicales et d’en faciliter l’accès au public désireux de les étudier, de les jouer et de les apprécier.

<br>  

<br>  


<!-- <br>  

<br>  


## Intensity of Scientific Activity: Highlights of Research Achievements During the FRQSC Emerging Team Grant (2014–2015)

<br>  

### Cantus Ultimus (Content Axis)

<br>  

The Cantus Ultimus project applies the latest OMR technologies to plainchant manuscripts in order to
transform the existing CANTUS database (directed by **Lacoste**) of nearly 400,000 chant records into a state-of-the-art research environment in which both music and text are fully searchable. Within the past year, we have processed and made publically available online two of the oldest surviving chant manuscripts, the late 10th-c. St. Gall manuscripts (CH-SGs 390 and 391), and completed the OMR of the Salzinnes antiphonal (cantus.simssa.ca/manuscripts/).

<br>  

### New Version of Diva (Content Axis)

<br>  

Diva (ddmal.github.io/diva.js/) is a web-based, open-source digital document viewer, developed by **Hankinson** (postdoctoral researcher), Wendy Liu, and Evan Magoni, and managed by **Fujinaga** and **Pugin**. Diva was designed for websites of libraries, archives, and museums so that they could present high-resolution images of documents in a user-friendly interface optimized for speed and flexibility. The new version, released in August 2015, supports the International Image Interoperability Framework (IIIF). The IIIF is an important new initiative committed to developing a set of common interfaces that support interoperability between image repositories, facilitating horizontal integration across libraries and archives all over the world.

<br>  

### Improved ELVIS Database (Content Axis)

<br>  

The ELVIS Database (elvisproject.ca/) is an open, crowd-sourced database of music in symbolic notation, maintained by MIRAI. This summer, undergraduate student Alex Parmentier improved the search capabilities of the database, making it more powerful and adaptable, and added new functions allowing users to upload new pieces and modify pieces they have uploaded. In collaboration with our project manager and graduate students in musicology, he clarified guidelines for data entry and made the interface more attractive and user friendly. Our continued building of the database (now totalling over 6,000 movements and pieces) has made large-scale corpus studies possible.

<br>  

### Increased Flexibility of the VIS Counterpoint Web Application (Searching & Analysis Axis)

<br>  

Our open-source software for analyzing counterpoint, VIS, and its corresponding web application (counterpoint.elvisproject.ca/) make large-scale corpus studies accessible to music theorists and musicologists without programming experience. Within the last year, we have made significant improvements to the stability and flexibility of VIS. In summer 2015, Ryan Bannon (undergraduate student and lead programmer on VIS) began integrating VIS into Rodan, a workflow engine developed by **Hankinson**. This integration involved the low-level code integration of VIS tasks into Rodan such that they can be realized as individual atomic workflow tasks, and the development of a user-friendly web application that allows researchers to generate VIS-based workflows via a graphical user interface. We are on schedule to release our web application in spring 2016.

<br>  

### New Version of jSymbolic (Searching & Analysis Axis)

<br>  

jSymbolic is a software tool developed by **McKay** for analyzing symbolic music files by extracting a range of characteristic statistical information (called “features”) relating to musical elements such as pitch, rhythm, harmony, instrumentation, dynamics, and texture. In 2015, **McKay** and Tristano Tenaglia (undergraduate student) developed a new version that can extract features from MEI files (Music Encoding Initiative: an open-source, computer-readable music encoding format) and from windowed sections of a score (rather than the score in its entirety). Additionally, jSymbolic is now able to export features into WEKA ARFF, which will be useful for machine learning, facilitating such tasks as composer, style, or genre recognition.

<br>  

### jMei2Midi (Searching & Analysis Axis)

<br>  

**McKay** has also developed a new Java application and library called jMei2Midi, which can convert MEI files to the widely recognized MIDI file format. This allows us to process music accessible only in MEI with software not yet able to read MEI files. jMei2Midi’s general parsing libraries will also provide a useful resource for developers in the process of incorporating MEI-parsing capabilities into their own software.

<br>  

## Intensity of Scientific Activity: Publications During the FRQSC Emerging Team Grant

<br>  

### Context Axis

<br>  

*Optical Music Recognition (OMR):* Charalampos, **Hankinson**, & **Fujinaga** 2014; **Helsen**, **Bain**, **Fujinaga**, **Hankinson**, & **Lacoste** 2014

*Digital Libraries:* **Cumming** 2014; **Fujinaga**, **Hankinson**, & **Cumming** 2014a; **Laplante**, **Hankinson**, **Cumming**, & **Fujinaga** 2015; **Pugin**, Zitellini, & Roland 2014; Roland, **Hankinson, & **Pugin** 2014

<br>  

### Searching and Analysis Axis

<br>  

*Analysis of Symbolic Scores:* Antila & **Cumming** 2014; **Cumming** & **Schubert** 2015a; Risk, Mok, **Hankinson**, & **Cumming** forthcoming; **Schubert** & **Cumming** 2015; Sigler, **Wild**, & Handelman forthcoming; Winters & **Cumming** 2014

*Corpus Study:* **Cumming** & **Schubert** 2015b

*Music Perception & Big Data:* **Fujinaga**, Sears, & **Hankinson** 2014b; Goebl, Bresin, & **Fujinaga** 2014; Siedenburg, Fujinaga, & McAdams 2014; Vigliensoni & Fujinaga 2014

*Prosopography:* **Fujinaga** 2014

*User Interface/User Experience:* **Bain**, Behrendt, & **Helsen** 2014

<br>  

## Intensity of Scientific Activity: Future Research Plans

<br>  

### Development of a Single Interface for Searching & Analysis

<br>  

As we continue to develop OMR technologies and grow our database of scores in symbolic notation (ELVIS), we will improve methods of searching and analyzing this data. This will involve harvesting metadata from our partner institutions (for details see p. 10, under SSHRC Partnership Grant) and linking it with the OMR data, allowing us, for example, to search for a particular melody in works from 1400–50 with Latin texts. Our goal is the equivalent of Google Books for music in both audio and symbolic formats.

In addition to the counterpoint web application, our postdoctoral researcher **Krämer** is working on a melody search tool that includes pattern recognition engines based on combinatorial mathematics; an automatic mode detector; state transition matrix generators for analyzing melody, harmony, and counterpoint; and visualization tools of state transition networks. Doctoral students, such as Alexander Morgan, have developed tools for identifying and indexing a variety of musical events ranging from simple notes and durations to complex chordal, contrapuntal, and syntactic patterns. **Rusch** and Bannon are developing tools for identifying cadential voice-leading strands. Although Rusch and Bannon’s tools are modelled on a corpus of Bach chorales, they will soon be applied to other repertoire.

What should a search tool look like for music? Should it be a keyboard, a textbox in which to input note letter names, or a music editor? As we address questions surrounding user interface and user experience, **Laplante’s** and **Chiasson-Taylor’s** expertise in the information behaviour of music researchers, librarians, and performers will be invaluable.

<br>  

### Large-Scale Corpus Research

<br>  

Already we have an impressive body of Renaissance scores in symbolic notation. We plan on continuing to expand the ELVIS Database in the next four years, which will facilitate research on how style changed from the 15th to the 17th centuries. Previous studies addressing such questions are based on an individual researcher’s study of a small corpus of works. With tools for large-scale searching of melody, harmony, and counterpoint, **Cumming**, **Schubert**, and **Ethier** will be able to provide a much more comprehensive account of changes to musical composition, such as the development of the tonal system. They are also planning on conducting large-scale corpus studies comparing composers and genres of the same era to discern differences in compositional practice, research that could be used to teach computers to automatically classify works by composer. In the next four years, we plan to expand the coverage of our database to other periods, facilitating corpus studies of other eras as well. This has already begun with **Rusch’s** work on Bach.

<br>  

### Music Encoding for Non-Standard Musical Notation Systems

<br>  

Continuing with the work of the Cantus Ultimus project (**Bain**, **Craig-Mcfeely**, **Cumming**, **Fujinaga**, **Helsen**, **Lacoste**, and **Long**) on plainchant neumes and **Pugin’s** work on MEI for 16th-century notation, we plan on developing music encoding for other non-standard musical notation systems. Together with postdoctoral researcher **Desmond**, an expert on 13th-century polyphony, **Cumming**, **Pugin**, **Weiss**, and **Wiering** are working on developing MEI for mensural notation (13th- to early 16th-century scores).

<br>  

### Feature Extraction and Machine Learning

<br>  

**McKay’s** jMIR software (jmir.sourceforge.net) provides a powerful infrastructure for extracting features from audio and symbolic musical sources and for processing this information with machine learning algorithms. The information extracted with these tools allows for sophisticated search capabilities for querying large music collections and provides a framework for automated music analysis. The process of integrating jMIR with other MIRAI components like VIS has already begun with jSymbolic. **McKay** also plans to expand the range of information extracted and generated by the jMIR components to meet the emerging needs of MIRAI, such as audio feature extraction, working closely with **McAdams** and **Traube**.

Another important area of future research is multimodal approaches to music processing, in other words, combining and processing information extracted from both symbolic and audio versions of the same work, as well as information from lyrics or images associated with the work. **McKay** has already published some initial research in this very promising new domain (e.g., McKay & Fujinaga 2008).

<br>  

### Biography & Linked Open Data

<br>  

Initially funded by the NEH Digital Humanities Start-Up Grant (PI: **Weiss**; Co-PI: **Fujinaga**), this project aims to create a framework that can answer questions not easily answered by a Google-like search; for example, Did composer A and composer B live in Rome in 1543? or Were there musicians working in Rome from 1540–45 who performed music by both of these composers? Although online biographical databases exist, they are expensive to build because they are created manually. We are reducing the costs of such endeavours by applying natural language processing, which automatically extracts necessary information from existing computer-readable documents, and using crowd-sourcing techniques to make corrections. The results will be stored in Linked Open Data databases. **Sinclair** (Professor of Digital Humanities) and historian **Milner** (Assistant Director, McGill Centre for Digital Humanities) will participate in this research.

<br>  

# II. Composition & Coordination

<br>  

## Skills, Leadership, and Experience of Lead Researcher: Ichiro Fujinaga

<br>  

Fujinaga is the team leader and head of the Content axis. He was one of first developers of OMR technology (Fujinaga 1988, 1996) and has published more papers than anyone else on the subject. In large part due to his research, OMR technology is now accurate enough to convert digitized images of scores into symbolic format, facilitating the work of the Searching and Analysis axis. Since commercial OMR software can only deal with modern music notation, for the past 8 years he has concentrated on extending OMR to early music notation from the 14th to the 17th centuries (e.g., Ouyang et al. 2009; Pugin et al. 2007a, 2007b, 2007c; Vigliensoni et al. 2011). Fujinaga is also one of the lead developers of the open-source document analysis software Gamera (based on his OMR software) (MacMillan et al. 2002a, 2002b, 2001; Pugin et al. 2008).

Fujinaga directs the Distributed Digital Music Archives & Libraries (DDMAL) research lab, consisting of over a dozen graduate and undergraduate students. He has extensive experience managing large teams of researchers. He was Acting Director of the Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT) from 2003–4, managing several large grants involving its over 30 faculty members and 100 graduate students across three Québec institutions. As the coordinator of the Steering Committee of the International Society of Music Information Retrieval, he has overseen the successful growth of a series of conferences on music information retrieval since 2001. He has been involved in many large joint research programs. A select listing includes an Intel Grant at John’s Hopkins University with over 20 researchers ($1.7M; 1999–2001); an NSF Information Technology Research Grant, involving 12 Co-PIs ($1.5M; 2002–6); an NSF Digital Libraries Initiative Grant with 8 other researchers ($0.5M; 1999–2002); a Community-University Research Alliances Grant from SSHRC with 8 other Co-PIs ($1M; 2005–10); a Major Collaborative Research Initiatives (MCRI) Grant from SSHRC with 16 other Co-PIs ($2.7M; 2007–13); and another MCRI with 41 other Co-PIs ($2.3M; 2009–15). As a PI, he has received nearly 5 million dollars in funding from SSHRC, CFI, and the FRQSC since arriving at McGill University in 2002.

<br>  

## Mechanisms to Coordinate the Team’s Work

<br>  

**Fujinaga** and **Cumming** coordinate the research and research dissemination of the Content and Searching & Analysis axes, respectively. Meetings and workshops are organized by the project manager. MIRAI members and their students meet weekly to present work in progress, establish new research projects, maintain our websites and social media presence, discuss additional funding opportunities, and recruit new members and students. With teleconferencing, we communicate regularly with collaborating members who reside outside of Montréal; for example, **Bain**, **Helsen**, and **Lacoste**, Co-PIs on the Cantus Ultimus project. With the support of the FRQSC Emerging Team Grant, we hosted the following events that brought us face-to-face with our international collaborators and disseminated our work to a wider audience:

* 7 July 2015: SIMSSA Workshop at the Medieval and Renaissance Music Conference; presentations by **Fujinaga**, **Cumming**, **Bain**, **Hankinson**, **Helsen**, **Lacoste**, and **Pugin**.
* 27 June 2015: SIMSSA Workshop at the joint meeting of the International Association of Music Libraries, Archives and Documentation Centres (IAML) and International Musicological Society (IMS); presentations by **Fujinaga**, **Cumming**, **Hankinson**, and **Pugin**.
* 7 November 2014: Workshop luncheon at the joint meeting of the American Musicological Society
(AMS) and the Society for Music Theory (SMT); presentations by **Fujinaga**, **Cumming**, and **Bain**.
* 6 November 2014: Working group and discussion of SIMSSA at the joint meeting of the AMS/SMT.
* 29 September 2014: CIRRMT Workshop (McGill) highlighting recent developments on the SIMSSA,
ELVIS, Cantus Ultimus, and Optical Neume Recognition projects; presentations by **Fujinaga**,
**Cumming**, **Bain**, **Hankinson**, and **Wild**.

<br>  

## Experience, Accomplishments, and Complementarity of Regular Team Members

<br>  

### Julie Cumming

<br>  

Cumming manages the Searching & Analysis axis. She is a leading figure in Renaissance music and digital musicology (2014, 2012). Her book (1999) is the definitive study of the motet from 1400–75. Since coming to McGill in 1992, she has been collaborating with **Schubert**. Together they have produced many collaboratively written papers (e.g., Cumming & Schubert 2015a, 2015b, 2015c; Schubert & Cumming 2012) and conference presentations, organized three conferences, and jointly supervised dozens of graduate theses. Cumming has been part of several large teams of researchers. She is the leader of one of the two primary research axes of the SSHRC Partnership Grant SIMSSA, lead by **Fujinaga**, and a Co-PI of the SSHRC Partnership Grant “Early Modern Conversions” ($2.5M; 2013–18), together with **Sinclair**. She has been a Co-PI on a SSHRC MCRI Grant entitled “Making Publics” ($2.5M; 2005–10), in which she developed an expertise on early music printing (Cumming 2010, 2012), and the PI of the Digging into Data Challenge Grant, “Electronic Locator of Vertical Interval Successions (ELVIS),” with 9 Co-PIs, including **Fujinaga** and **Schubert**, as well as 17 students. The ELVIS project broke new ground in computerized music analysis. She has worked closely with **Fujinaga** in the development of OMR for Renaissance music prints and manuscripts, and for plainchant. Cumming currently supervises 2 postdoctoral researchers, 8 doctoral students, and 2 master’s students. Several of her students work in **Fujinaga’s** DDMAL lab.

<br>  

### Peter Schubert

<br>  

As an expert on counterpoint pedagogy, Schubert and will manage (with **Ethier**) the counterpoint analysis project within the Searching & Analysis axis. He has published two textbooks on counterpoint—one on 16thcentury counterpoint (1999) and one on 18th-century counterpoint with Christoph Neidhöfer (2006)—two chapters on historical music pedagogy, and many articles on contrapuntal structures in the music of Willaert, Lassus, and Palestrina. He has collaborated frequently with **Cumming** on papers in which they show how knowledge of improvised counterpoint changes the way we think of compositional technique in the Renaissance. He is a member of CIRMMT, having collaborated on two experimental projects, one on the perception of Renaissance variation techniques with **McAdams**, and one on historical tuning systems with **Fujinaga** and **Wild** (Devaney et al. 2013, 2010). He has worked on computer modeling of counterpoint as a Co-PI on the SSHRC Partnership Grant SIMSSA, lead by **Fujinaga**. Schubert also brings to MIRAI extensive experience as a performer. Currently, he conducts three vocal ensembles: the Schulich School Singers, Les Chanteurs d’Orphée, and VivaVoce (which released a CD on the Naxos label in 2007).

<br>  

### Audrey Laplante

<br>  

Laplante’s research concerns music information-seeking behaviour and music information retrieval systems (Desrochers et al. 2013; Laplante 2015, 2014a, 2014b, 2014c, 2013, 2010b; Laplante et al. 2011). Laplante will co-lead (with **Chiasson-Taylor**) the user interface project, assisting both axes in their development of user-centred interfaces. She is a Co-PI on the SSHRC Partnership Grant SIMSSA, lead by **Fujinaga**; and the PI on the SSHRC Insight Development Grants “Chercheurs 2.0: Étude des usages et des facteurs d’adoption des médias sociaux par les chercheurs canadiens en contexte de collaboration interinstitutionnelle” ($74,028; 2015–17) and “L’indexation de musique à grande échelle : exploration du potentiel de nouvelles méthodes d'indexation pour accroître la visibilité de la chanson francophone sur le Web” ($69,735; 2012–14) (Kessler et al. 2014a, 2014b). Her prior work focused on the music information-seeking behaviour of young adults (2012, 2011, 2010a). By examining how young adults discover new music and interact with current music information sources, she was able to identify behaviours that were not well supported by current systems and to provide specific recommendations for the design of better music information systems. Prior to joining the Université de Montréal, Laplante worked as a music librarian at Concordia University, which allowed her to work closely with music scholars, acquiring a good understanding of their information needs.

<br>  

### Cory McKay

<br>  

McKay’s expertise in multimodal music analysis and processing will be put to use in the Searching and Analysis axis. His previous work as project leader for the jMIR project has provided him with extensive experience producing software that can collect, analyze, organize and process diverse kinds of musical information, both in combination and separately. McKay’s contributions to MIRAI will focus on researching techniques and developing software for extracting metadata from musical scores and audio recordings, and making this information accessible and useful to music researchers. During his doctoral studies, McKay worked in the DDMAL lab and **Fujinaga** was his supervisor. Since his graduation in 2010, McKay and **Fujinaga** have continued to collaborate, publishing over 24 peer-reviewed papers (Fiebrink et al. 2005; Fujinaga et al. 2008; McEnnis et al. 2006a, 2006b, 2005; McKay et al. 2010a, 2010b, 2009, 2012, 2009a, 2009b, 2008, 2007, 2006a, 2006a, 2006b, 2006c, 2005a, 2005b, 2005c, 2004; Thompson et al. 2009; Vigliensoni et al. 2010). McKay is a Co-PI on the SSHRC Partnership Grant SIMSSA, lead by **Fujinaga**.

<br>  

### Caroline Traube

<br>  

Traube will contribute to the Audio branch of the Searching & Analysis axis. She specializes in musical acoustics, specifically the analysis of musical performance parameters, such as timing variation, intonation, dynamics, and timbre (Bel et al. 2015; Bernays et al. 2014, 2013a, 2013b, 2012, 2011, 2010; Tillmann et al. 2011; Traube 2015, 2013). She is the founder of the Laboratoire de recherche sur le geste musicien (LRGM) at the Université de Montréal, which gathers performers, composers, musicologists, and scientists to tackle research questions relating to the gestural control and the verbal description of timbre and the analogies between the prosodic features of speech and the expressive parameters of instrumental performance. As part of MIRAI, she will develop tools to generate a complete a musical score with performance parameters that are not notated as well as information on the gestural control of instrumental sound.

<br>  

### Glen Ethier

<br>  

Ethier will collaborate with **Schubert** on the counterpoint analysis project and with both **Schubert** and **Cumming** on their research into style change and the development of the tonal system. His prior work has focused on the perception of melodic accent and the development of an analytic methodology premised on the interaction and concordance of accent in counterpoint of the early 16th century. He has developed paradigms for defining and quantizing strengths for different types of melodic accent and applying these paradigms to re-notated versions of contrapuntal scores. With MIRAI, Ethier will create computer-based models to quantize the parameters for analysis over a much broader corpus of works than he was able to examine in his prior work, including early Renaissance counterpoint and 18th-century tonal counterpoint. Ethier completed his doctoral studies at McGill under Schubert.

<br>  

### Rachelle Chiassion-Taylor

<br>  

Together with **Laplante**, Chiasson-Taylor will manage the user interface component of the MIRAI research program. She is currently Senior Music Archivist at Library and Archives Canada (LAC) and a Guest Professor of Musicology at the Université de Montréal Faculty of Music, where she teaches early music history and research methodology. At LAC, she has been closely involved in the development of digital capacity and processing procedures for the institution, which is designated as a Trusted Digital Repository, and has processed a number of born-digital music archives using a variety of non-proprietary software and complex preservation procedures. She also participates in a LAC study group on the archiving and retrieval of complex digital objects and has given papers on digital music processing within archival fonds. In addition to digital archives processing and web harvesting strategies, she brings to MIRAI considerable practical experience and knowledge of metadata standards in federal institutions internationally. Chiasson-Taylor has published collections of essays on early keyboard music, several journal articles on a variety of topics including digital music archives at LAC, the Canadian composer István Anhalt, and the involvement of musicians in espionage activities in Early Modern Europe (Smith & Taylor ed. 2013; Taylor & Knox ed. forthcoming; Taylor forthcoming, 2013, 2011, 2002, 2000). She is also an internationally recognized performer and has recorded three CDs of Renaissance keyboard music for the ATMA label. She brings archival science, historical musicology, and early music performance to the MIRAI team. **Cumming** was the adviser of Chiasson-Taylor’s doctoral degree in musicology at McGill.

<br>  

## Complementarity of Team Members’ Expertise and Quality of Their Collaborations

<br>  

The success of our research program is ensured, in part, by the members of our team, due to the diversity of disciplines they represent and their rich assortment of skills. **Fujinaga**, **McKay**, and **Traube** are music technologists, **Cumming** is a historical musicologist, **Schubert** and **Ethier** are music theorists, **Laplante** is an information scientist, and **Chiasson-Taylor** is an archivist, musicologist, and performer. The members of the MIRAI team are at different stages of their academic careers: **Schubert** is a full professor; **Cumming**, **Fujinaga**, **Laplante**, and **Traube** are associate professors; **McKay** and **Ethier** are tenured faculty members at CÉGEPs; and **Chiasson-Taylor** is Senior Music Archivist for Library and Archives Canada. What brings us together is a shared commitment to creating a music library for the 21th century, a commitment amply demonstrated by our many collaboratively produced publications and conference presentations.

Many of MIRAI’s regular team members have a long history of successful research collaborations. The most mature partnership involves **Cumming** and **Schubert**, who have been working together since the early 1990s on style change, performance practice, and compositional process in Renaissance music. Their work has been supported by three SSHRC Standard Research grants (1995, 2001, 2009). **Fujinaga** has worked with Cumming on optical music recognition of early music since 2005. Since 2007, he has worked with **Schubert** on tuning in ensemble singing; a research funded by an FRQSC Recherche-Création Grant.

<br>  

# III. Student Integration

<br>  

MIRAI will support 8 students per year (research assistants: 2 undergraduates and 2 MA students each year; scholarships: 2 MA and 2 PhD students each year) from a range of disciplines: musicology, music theory, music technology, computer science, and software engineering. Funding from the FRQSC will allow us to hire 2 undergraduate research assistants in computer science each summer, supervised by **Fujinaga** and **McKay**, to maintain our databases (ELVIS, OMR, and audio) and websites (simssa.ca, elvisproject.ca, cantus.simssa.ca). We will also hire 2 MA research assistants in music theory and musicology each year, supervised by **Cumming**, **Schubert**, and **Ethier**, to assist in software testing and to advise the software programmers as specialist users. These students will also help to disseminate our research via both scholarly and social media outlets (e.g., blogs, Twitter, and Facebook).

We will provide scholarships to support thesis research for 2 master’s students and 2 doctoral students. Musicology and music theory students will be involved in the Searching & Analysis axis by using the tools MIRAI team members are developing, such as VIS, in their thesis research and collaborating with the software developers to improve usability. Music technology graduate students will be involved in the Content axis, helping to improve OMR technology and the searching and analysis tools. Over two-thirds of our funding from other sources such as SSHRC and NEH is budgeted for student work.

MIRAI funding from FRQSC will help us to support the salary for one of our postdoctoral researchers each year, compensating him or her for the time spent on administrative work, such as assisting in grant applications, supervising student workers, overseeing databases, and running meetings.

The students and the postdoc involved in the MIRAI research program will work in a research lab with students at all levels and professors, interacting daily as questions and problems arise. As part of a large-scale multi-disciplinary team they will learn new ways at looking at the world and new ways of organizing information. For example, computer scientists will learn about music history, and music theorists will learn how to communicate with computers and programmers. Weekly meetings of the research axes will provide opportunities for all the students to present their work, to comment on the work of others, and to learn from each other. Students will also have the opportunity to present at workshops and team meetings. In Years 1 and 3, we will fund 2–3 students to assist with the MIRAI Workshop at the joint conference of the American Musicological Society and Society for Music Theory. The skills they develop—working with a diverse team, presenting their work in clear and effective ways, participating in the development of software and user interfaces—will be of great benefit no matter what field they go into. All of them will be contributing to important research that has the potential to change the way people interact with music.

<br>  

# IV. Added Value of the Grant

<br>  

## Team Needs

<br>  

Funding MIRAI at the operational level will allow us to 1) purchase the hardware necessary to conduct our data-intensive research and back-up our databases; 2) coordinate our multiple parallel projects and help us to further expand and deepen these collaborations, especially with international collaborators; 3) motivate graduate students to join our team; and 4) disseminate our work to a wider audience through workshops at McGill University (in English) and the Université de Montréal (bilingual) as well as at conferences in the fields of musicology, music theory, music performance, library science, and computer science.

<br>  

### Equipment

<br>  

Given the data-intensive nature of this research program, adequate numbers of computers for our students, and digital backup and storage of our data are essential. The laptops requested will be used by the project manager, student workers as well as for the CEGEP researchers. In the coming years, our server needs will also increase, especially with serving audio files.

<br>  

### Coordination

<br>  

Funding from the FRQSC would allow us to continue to afford a half-time project manager to assist lead researcher in coordinating MIRAI’s multiple parallel projects, meetings, and workshops. Given the long-term nature of many of our projects, we also require documentation to facilitate the seamless transfer of knowledge from successive generations of students and users. This documentation will be generated by the students and postdoctoral researchers, coordinated by the lead researcher and project manager.

<br>  

### Dissemination

<br>  

Funding from FRQSC will also facilitate face-to-face meetings among our team members. We plan to host at least one workshop per year that brings together all regular and collaborating members:

* May 2016: Music Encoding Conference, McGill University
* November 2016: Joint meeting of the American Musicological Society (AMS) and Society for Music Theory (SMT) in Vancouver, BC
* May 2017: McGill University (with funding from CIRMMT)
* August 2017: Alliance of Digital Humanities Organizations, McGill University
* 2017: Conference on Complex Digital Objects, Ottawa, ON
* November 2018: Joint meeting of the AMS and SMT in San Antonio, TX
* May 2019: Université de Montréal (bilingual)

These workshops will be an opportunity not only for collaboration between regular and collaborating members and their students but also for sharing our work with a wider audience of scholars, librarians, and performers, both local and international.

<br>  

## Other Resources to which the Team Has Access

<br>  

Funding from the FRQSC would complement the funding we are receiving from various other sources. Within the last five years, over $3,300,000 has been allocated towards this research program:

* 2014–21: SSHRC Partnership Grant, “Single Interface for Music Score Searching and Analysis (SIMSSA)” ($2,499,197 plus $227,500 from McGill University). PI: **Fujinaga**; Co-PIs: **Bain**, **Cumming**, **Helsen**, **Lacoste**, **Laplante**, Leive, **McKay**, **Pugin**, Rodin, **Rusch**, **Schubert**, Tzanetakis, **Wild**. Partners: Alexander Street Press; Bavarian State Library; Bibliothèque nationale de France; British Library; Compute Canada; Dalhousie University; Digital Image Archive of Medieval Music, Oxford University; Harvard University Music Library; New York Philharmonic Archives; Marenzio Project, University of Pennsylvania; Répertoire International de Littérature Musicale: Répertoire International des Sources Musicales-Switzerland; The Juilliard School; The Walters Art Museum; Université de Montréal; HathiTrust Research Center, University of Illinois at Urbana-Champaign; University of London, Goldsmiths; University of Pennsylvania Libraries, University of Victoria, University of Virginia, Music Encoding Initiatives; University of Washington Music Library; University of Waterloo.
* 2014–19: SSHRC Insight Grant, “Cantus Ultimus: Building the Ideal Online Plainchant Research
Environment” ($499,566). PI: **Fujinaga**; Co-PIs: **Cumming**, **Bain**, **Helsen**, **Lacoste**.
* 2014–16: FRQSC Emerging Team Grant, “D’information musicale, de la recherche, et de l’infrastructure : Construction de la bibliothèque musicale mondiale du 21e siècle” ($54,963). PI: **Fujinaga**; Co-PIs: **Cumming**, **Laplante**, **McKay**, **Schubert**.
* 2013–15: NEH Digital Humanities Start-up Grant Level-II, “Digital Prosopography for Renaissance Musicians: Discovery of Social and Professional Network” ($54,466US) PI: **Weiss**. Co-PI: **Fujinaga**.

To further support our research program, we have applied for a SSHRC Connection Grant to host the Music Encoding Conference at McGill in May 2016 and to Compute Canada for servers (submitted October 2015). Upcoming grant applications include the NEH Digital Implementation Grant ($100,000–325,000US) to support **Fujinaga**, **Milner**, and **Weiss**’s digital prosopography project (January 2016). In September 2016, **Cumming** and **Schubert** will be submitting a joint application for a SSHRC Insight Grant to support their work tracing improvisatory patterns in polyphonic music from 1400–1750.

FRQSC funding will allow us to continue building and strengthening our team as it builds the infrastructure to support a 21th-century global music library.

<br>  

# Bibliography

<br>  

Angeles, B., C. McKay, and I. Fujinaga. 2010. “Discovering Metadata Inconsistencies.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 195–200. Utrecht, Netherlands.

Antila, C., and J. Cumming. 2014. “The VIS Framework: Analyzing Counterpoint in Large Datasets.” In In *Proceedings of the International Society for Music Information Retrieval*, 71–76. Taipei, Taiwan.

Bain, J., I. Behrendt, and K. Helsen. 2014. “Linienlose Neumen, Neumentrennung und Repräsentation von Neumen mit MEI Schema –Herausforderungen in der Arbeit im Optical Neume Recognition Project (ONRP).” [Staffless Neumes, Neume Separation and Representation of Neumes with the Help of MEI – Challenges in the Optical Neume Recognition Project (ONRP)] In *Digitale Rekonstruktionen mittelalterlicher Bibliotheken*, edited by S. Philippi and P. Vanscheidt,. Trierer Beiträge zu den historischen Kulturwissenschaften 12: 119–32. Wiesbaden: Ludwig Reichert.

Bel, S., and C. Traube. 2015. “Corrélats acoustiques de cinq nuances de timbre au piano.” In *Actes des Journées d’informatique musicale*. Montréal, QC.

Bernays, M., and C. Traube. 2014. “Investigating Pianists’ Individuality in the Performance of Five Timbral Nuances through Patterns of Articulation, Touch, Dynamics, and Pedaling.” *Frontiers in Psychology – Cognitive Science* 5, article 157: 35–53.

Bernays, M., and C. Traube. 2013a. “Expression of Piano Timbre: Verbal Description and Gestural Control.” In *La musique et ses instruments / Music and its instruments*, ed. M. Castellengo and H. Genevois, 205–22. Sampzon: Éditions Delatour-France.

Bernays, M., and C. Traube. 2013b. “Expressive Production of Piano Timbre: Touch and Playing Techniques for Timbre Control in Piano Performance.” In *Proceedings of Stockholm Music Acoustics Conference / Sound and Music Computing Conference*, 341–46. Stockholm, Sweden.

Bernays, M., and C. Traube. 2012. “Piano Touch Analysis: A MATLAB Toolbox for Extracting Performance Descriptors from High-Resolution Keyboard and Pedalling Data.” In *Actes des Journées d’informatique musicale*, 55–64. Mons, Belgium.

Bernays, M., and C. Traube. 2011. “Verbal Expression of Piano Timbre: Multidimensional Semantic Space of Adjectival Descriptors.” In *Proceedings of the International Symposium on Performance Science*, 299–304. Toronto, ON.

Bernays, M., and C. Traube. 2010. “Expression of Piano Timbre: Gestural Control, Perception and Verbalization.” In *Proceedings of the International Conference on Music Perception and Cognition*. Seattle, WA.

Burgoyne, J. A., J. Wild, and I. Fujinaga. 2013. “Compositional Data Analysis of Harmonic Structures in Popular Music.” In *Proceedings of the International Conference on Mathematics and Computation in Music*, 52–63. Montréal, QC.

Burgoyne, J. A., J. Wild, and I. Fujinaga. 2011. “An Expert Ground Truth Set for Audio Chord Recognition and Music Analysis.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 633–38. Miami, FL.

Burgoyne, J. A., Y. Ouyang, T. Himmelman, J. Devaney, L. Pugin, and I. Fujinaga. 2009. “Lyric Extraction and Recognition on Digital Images of Early Music Sources.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 723–28. Kobe, Japan.

Burgoyne, J. A., J. Devaney, L. Pugin, and I. Fujinaga. 2008. “Enhanced Bleedthrough Correction for Early Music Documents with Recto-verso Registration.” In *Proceedings of the International Conference on Music Information Retrieval*, 407–12. Philadelphia, PA.

Burgoyne, J. A., L. Pugin, G. Eustace, and I. Fujinaga. 2007a. “A Comparative Survey of Image Binarisation Algorithms for Optical Recognition on Degraded Musical Sources.” In *Proceedings of International Conference on Music Information Retrieval*, 509–12. Vienna, Austria..

Burgoyne, J. A., L. Pugin, C. Kereliuk, and I. Fujinaga. 2007b. “A Cross-Validated Study of Modelling Strategies for Automatic Chord Recognition in Audio.” In *Proceedings of International Conference on Music Information Retrieval*, 251–4. Vienna, Austria.

Burlet, G., A. Porter, A. Hankinson, and I. Fujinaga. 2012. “Neon.js: Neume Editor Online.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 121–26. Porto, Portugal.

Charalampos, S., A. Hankinson, and I. Fujinaga. 2014. “Correcting Large-Scale OMR Data with Crowdsourcing.” In *Proceedings of the International Workshop on Digital Libraries for Musicology*, 88–90. London, UK.

Cumming, J., and P. Schubert. 2015a. “Another Lesson from Lassus: Using Computers to Analyze Counterpoint.” *Early Music* 43 (2015).

Cumming, J., and P. Schubert. 2015b. “The Origins of Pervasive Imitation.” In *The Cambridge History of Fifteenth-Century Music*, edited by A. M. Busse Berger and J. Rodin, 200–28. Cambridge: Cambridge University Press.

Cumming, J., and P. Schubert. 2015c. “Talking About the Lost Generation: Sacred Music of Willaert, Gombert, and Michele Pesenti.” Introduction to a Special Issue of *Journal of Musicology* 32: 323–27.

Cumming, J. 2014. “The Past Is Not Over: Special Collections in the Digital Age.” In *Meetings with Books: Symposium on Special Collections in the 21st Century. With a Tribute to Raymond Klibansky and an Illustrated Survey of McGill Library Special Collections*, edited by J. Tomm and R. Virr, 109–14. Montréal: McGill University Library.

Cumming, J. 2012. “Text Setting and Imitative Technique in Petrucci’s First Five Motet Prints.” In *The Motet around 1500: On the Relationship of Imitation and Text Treatment*, edited by T. Schmidt-Beste, 63–90. Turnhout: Brepols.

Cumming, J. 2010. “Petrucci’s Publics for the First Motet Prints.” In *Making Publics in Early Modern Europe: People, Things, Forms of Knowledge*, edited by B. Wilson and P. Yachnin, 96–122. New York and London: Routledge.

Cumming, J. 1999. *The Motet in the Age of Du Fay*. Cambridge: Cambridge University Press.

Desrochers, N., A. Laplante, Quan-Haase, A., Martin, K., Rasmussen, D., & Spiteri, L. 2013. “Beyond the Playlist: Looking at User-Generated Collocation of Cultural Products Through Social Tagging.” In *Proceedings of the 76th Annual Meeting of the Association for Information Science and Technology*. Montréal, QC.

Devaney, J., J. Hockman, J. Wild, P. Schubert, and I. Fujinaga. 2013. “Diatonic Semitone Tuning in Twopart Singing.” In *Conference Program of the Society of Music Perception and Cognition Conference*, 43. Toronto, ON.

Devaney, J., M. I. Mandel, D. P. W. Ellis, and I. Fujinaga. 2011a. “Automatically Extracting Performance Data from Recordings of Trained Singers.” *Psychomusicology: Music, Mind & Brain* 21, nos. 1–2: 108–36.

Devaney, J., M. I. Mandel, and I. Fujinaga. 2011b. “Characterizing Singing Voice Fundamental Frequency Trajectories.” In *Proceedings of the Workshop on Applications of Signal Processing to Audio and Acoustics*, 73–76. New Paltz, NY.

Devaney, J., J. Wild, and I. Fujinaga. 2011c. “Intonation in Solo Vocal Performance: A Study of Semitone and Whole Tone Tuning in Undergraduate and Professional Sopranos.” In *Proceedings of the International Symposium on Performance Science*, 219–24. Toronto, ON.

Devaney, J., J. Wild, P. Schubert, and I. Fujinaga. 2010. “Exploring the Relationship Between Voice
Leading, Harmony, and Intonation in a cappella SATB Vocal Ensembles.” In *Proceedings of the
International Conference on Music Perception and Cognition*, 315–16. Seattle, WA.

Fiebrink, R., C. McKay, and I. Fujinaga. 2005. “Combining D2K and JGAP for Efficient Feature Weighting for Classification Tasks in Music Information Retrieval.” In *Proceedings of the International Conference on Music Information Retrieval*, 510–13. London, UK.

Fujinaga, I. 2014. “Digital Prosopography of Renaissance Musicians: A Progress Report.” In *Program and Abstract Book of the Annual Meeting of the Renaissance Society of America*, 199. New York, NY.

Fujinaga, I., A. Hankinson, and J. Cumming. 2014a. “Introduction to SIMSSA (Single Interface for Music Score Searching and Analysis).” In *Proceedings of the International Workshop on Digital Libraries for Musicology*, 100–2. London, UK.

Fujinaga, I., D. Sears, and A. Hankinson. 2014b. “Big Data for the Music Perception and Cognition Community.” In *Book of Abstracts of International Conference on Music Perception and Cognition - Asia-Pacific Society for the Cognitive Sciences of Music Joint Conference*, 78. Seoul, South Korea.

Fujinaga, I. and A. Hankinson. 2013. “SIMSSA: Towards Full-Music Search Over a Large Collection of Musical Scores.” In *Conference Abstracts of Digital Humanities*, 187–89. Lincoln, NE.

Fujinaga, I., and C. McKay. 2008. “ACE: Autonomous Classification Engine.” In *Proceedings of the
International Conference on Music Perception and Cognition*. Sapporo, Japan.

Fujinaga, I. 2004. “Staff Detection and Removal.” In *Visual Perception of Music Notation*, ed. S. George, 1–39. Hershey, PA: Idea Group Inc.

Fujinaga, I., and K. MacMillan. 2000. “Realtime Recognition of Orchestral Instruments.” In *Proceedings of the International Computer Music Conference*, 141–43. Berlin, Germany.

Fujinaga, I. 1996. “Adaptive Optical Music Recognition”. Ph.D. Dissertation, McGill University.

Fujinaga, I. 1988. “Optical Music Recognition Using Projections.” M.A. Thesis, McGill University.

Goebl, W., R. Bresin, and I. Fujinaga. 2014. “Perception of Touch Quality in Piano Tones.” *Journal of the Acoustical Society of America* 136: 2839–50.

Hankinson, A. 2014. “Optical Music Recognition Infrastructure for Large-scale Music Document Analysis.” Ph.D. Dissertation, McGill University.

Hankinson, A., J. A. Burgoyne, G. Vigliensoni, A. Porter, J. Thompson, W. Liu, R. Chiu, and I. Fujinaga. 2012a. “Digital Document Image Retrieval Using Optical Music Recognition.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 577–82. Porto, Portugal.

Hankinson, A., J. A. Burgoyne, G. Vigliensoni, and I. Fujinaga. 2012b. “Creating a Large-scale Searchable Digital Collection from Printed Music Materials.” In *Proceedings of the Advances in Music Information Research*, 903–8. Lyon, France.

Hankinson, A., W. Liu, L. Pugin, and I. Fujinaga. 2012c. “Diva: A Web-based High-resolution Digital
Document Viewer.” In *Proceedings of the Theory and Practice of Digital Libraries Conference*, 455–60. Paphos, Cyprus.

Hankinson, A., and I. Fujinaga. 2010. “An Interchange Format for Optical Music Recognition Applications.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 51–56. Utrecht, Netherlands.

Hankinson, A., L. Pugin, and I. Fujinaga. 2009. “Interfaces for Document Representation in Digital Music Libraries.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 39–44. Kobe, Japan.

Helsen, K., J. Bain, I. Fujinaga, A. Hankinson, and D. Lacoste. 2014. “Optical Music Recognition and Manuscript Chant Sources.” *Early Music* 42, no. 4: 555–58.

Kessler, R., A. Laplante, A., and D. Forest. 2014a. “Exploration d’une collection de chansons à partir d'une interface de visualisation basée sur une analyse des paroles.” In *Revue des nouvelles technologies de l'nnformation*, edited by C. Reynaud, A. Martin, and R. Quiniou, 347–352.

Kessler, R., D. Forest, and A. Laplante. 2014b. “Encore des mots, toujours des mots : fouille de textes et visualisation de l’information pour l’exploration et l’analyse d’une collection de chansons en français.” In *Actes des 12e Journées internationales d’analyse statistique des données textuelle*, edited by J.-M. Daube, M. Valette, and S. Fleury, 311–322.

Laplante, A. 2015. “Tagged at First Listen: An Examination of Social Tagging Practices in a Music Recommender System.” *Encontros Bibli: Revista electronica de biblioteconomia e ciência da informação* 20, no. 1: 33–54.

Laplante, A., A. Hankinson, J. Cumming, and I. Fujinaga. 2015. “SIMSSA : Une interface unique pour la recherche et l’analyse de millions de partitions musicales numériques.” In *Actes des Journées d’informatique musicales*. Association Française d’Informatique Musicale.

Laplante, A. 2014a. “Le comportement informationnel des élèves du secondaire à l’ère des médias sociaux.” In Actes du colloque Autour de l'adulte de demain: Bibliothèque et Archives nationales du Québec, edited by P. Grenier and M.-C. Beaudry. http://www.banq.qc.ca/documents/activites/colloque/2012-2013/enfant_litterature/Laplante.pdf

Laplante, A. 2014b. “Social Capital and Academic Help Seeking: Late Adolscents’ Use of People as Information Sources.” In *New Directions in Children’s and Adolscents’ Information Behaviour Research* 10, edited by D. Bilal and J. Beheshti, 67–103. Emerald Group Publishing Limited.

Laplante, A. 2014c. “Improving Music Recommender Systems: What Can We Learn from Research on Music Tastes?” In *Proceedings of the 15th Conference of the International Society for Music Information Retrieval*, 451–46. Taipei, Taiwan.

Laplante, A. 2013. “Les bibliothèques universitaires québécoises et la génération C.” *Documentation et Bibliothèques* 59, no. 2: 91–101.

Laplante, A. 2012. “Who Influence the Music Tastes of Adolescents? A Study on Interpersonal Influence in Social Networks.” In *Proceedings of the second international ACM workshop on Music Information Retrieval with User-Centered and Multimodal Strategies*, 37–42. New York, NY.

Laplante, A. 2011. “Social Capital and Music Discovery: An Examination of the Ties Through which Late Adolescents Discover New Music.” In *Proceedings of the 12th International Society for Music Information Retrieval Conference*, 341–46. Miami, FL.

Laplante, A., and J. S. Downie. 2011. “The Utilitarian and Hedonic Outcomes of Music Information Seeking in Everyday Life.” *Library and Information Science Research* 33, no. 3: 202–10.

Laplante, A. 2010a. “The Role People Play in Adolescents’ Music Information Acquisition.” In *Proceedings of the Workshop on Music Recommendation and Discovery*.

Laplante, A. 2010b. “Users’ Relevance Criteria in Music Retrieval in Everyday Life: An Exploratory Study.” In *Proceedings of the 11th International Society for Music Information Retrieval Conference*, 601–6. Utrecht, Netherlands.

MacMillan, K, M. Droettboom, and I. Fujinaga. 2002a. “Gamera: Optical Music Recognition in a New Shell.” In *Proceedings of the International Computer Music Conference*, 482–85. Göteborg, Sweden.

MacMillan, K., M. Droettboom, and I. Fujinaga. 2002b. “Gamera: A Python-based Toolkit for Structured Document Recognition.” In *Proceedings of Tenth International Python Conference*, 25–40. Alexandria, VA.

MacMillan, K., M. Droettboom, and I. Fujinaga. 2001. “Gamera: A Structured Document Recognition Application Development Environment.” In *Proceedings of the International Symposium on Music Information Retrieval*, 15–16. Bloomington, IN.

McEnnis, D., C. McKay, and I. Fujinaga. 2006a. “jAudio: Additions and Improvements.” In *Proceedings of the International Conference on Music Information Retrieval*, 385–86. Victoria, BC.

McEnnis, D., C. McKay, and I. Fujinaga. 2006b. “Overview of OMEN.” *Proceedings of the International Conference on Music Information Retrieval*, 7–12. Victoria, BC.

McEnnis, D., C. McKay, I. Fujinaga, and P. Depalle. 2005. “Feature Extraction: An Extensible Library Approach.” *Proceedings of the International Conference on Music Information Retrieval*, 600–3. London, UK.

McKay, C. and I. Fujinaga. 2012. “Expressing Musical Features, Class Labels, Ontologies, and Metadata Using ACE XML 2.0.” In *Structuring Music through Markup Language: Designs and Architectures*, ed. J. Steyn, 48–79. Hershey, PA: IGI Global.

McKay, C., J. A. Burgoyne, J. Hockman, J. Smith, and I. Fujinaga. 2010a. “Evaluating the Performance of Lyrical Features Relative to and in Combination with Audio, Symbolic and Cultural Features.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 213–18. Utrecht, Netherlands.

McKay, C., and I. Fujinaga. 2010b. “Improving Automatic Music Classification Performance by Extracting Features from Different Types of Data.” *Proceedings of the ACM International Conference on Multimedia Information Retrieval*, 257–66.

McKay, C., and I. Fujinaga. 2009a. “jMIR: Tools for Automatic Music Classification.” In *Proceedings of the International Computer Music Conference*, 65–68. Montreal, QC.

McKay, C., J. A. Burgoyne, J. Thompson, and I. Fujinaga. 2009b. “Using ACE XML 2.0 to Store and Share Feature, Instance and Class Data for Musical Classification.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 303–8. Kobe, Japan.

McKay, C., and I. Fujinaga. 2008. “Combining Features Extracted from Audio, Symbolic and Cultural Sources.” In *Proceedings of the International Conference on Music Information Retrieval*, 597–602. Philadelphia, PA.

McKay, C., and I. Fujinaga. 2007. “jWebMiner: A Web-based Feature Extractor.” *Proceedings of the International Conference on Music Information Retrieval*, 113–14. Vienna, Austria.

McKay, C., and I. Fujinaga. 2006a. “jSymbolic: A Feature Extractor for MIDI Files.” *Proceedings of the International Computer Music Conference*, 302–5. New Orleans, LA.

McKay, C., D. McEnnis, and I. Fujinaga. 2006b. “A Large Publicly Accessible Prototype Audio Database for Music Research.” *Proceedings of the International Conference on Music Information Retrieval*, 160–63. Victoria, BC.

McKay, C., and I. Fujinaga. 2006c. “Musical Genre Classification: Is it Worth Pursuing and How Can it Be Improved?” In *Proceedings of the International Conference on Music Information Retrieval*, 101–6. Victoria, BC.

McKay, C., R. Fiebrink, D. McEnnis, B. Li, and I. Fujinaga. 2005a. “ACE: A Framework for Optimizing Music Classification.” In *Proceedings of the International Conference on Music Information Retrieval*, 42–49. London, UK.

McKay, C., D. McEnnis, R. Fiebrink, and I. Fujinaga. 2005b. “ACE: A General-purpose Classification Ensemble Optimization Framework.” In *Proceedings of the International Computer Music Conference*, 161–64. Barcelona, Spain.

McKay, C., and I. Fujinaga. 2005c. “Automatic Music Classification and the Importance of Instrument Identification.” In *Proceedings of the Conference on Interdisciplinary Musicology*, 87–89. Montreal, Canada.

McKay, C., and I. Fujinaga. 2004. “Automatic Genre Classification Using Large High-level Musical Feature Sets.” In *Proceedings of the International Conference on Music Information Retrieval*, 525–30. Barcelona, Spain.

Ouyang, Y., J. A. Burgoyne, L. Pugin, and I. Fujinaga. 2009. “A Robust Border Detection Algorithm with Application to Medieval Music Manuscripts.” In *Proceedings of the International Computer Music Conference*, 101–4. Montreal, QC.

Pugin, L., R. Zitellini, and P. Roland. 2014. “Verovio: A Library for Engraving MEI Music Notation into SVG.” In *Proceedings of the 15th International Society for Music Information Retrieval Conference*, 107–12. Taipei, Taiwan.

Pugin, L., J. Hockman, J.A. Burgoyne, and I. Fujinaga. 2008. “Gamera versus Aruspix: Two Optical Music Recognition Approaches.” In *Proceedings of the International Conference on Music Information Retrieval*, 419–24. Philadelphia, PA.

Pugin, L., J. A. Burgoyne, and I. Fujinaga. 2007a. “MAP Adaptation to Improve Optical Music Recognition of Early Music Documents Using Hidden Markov Models.” In *Proceedings of International Conference on Music Information Retrieval*, 513–16. Vienna, Austria.

Pugin, L., J. A. Burgoyne, and I. Fujinaga. 2007b. “Goal-directed Evaluation for the Improvement of Optical Music Recognition of Early Music Prints.” In *Proceedings of the Joint Conference on Digital Libraries*, 303–4. Vancouver, BC.

Pugin, L., J. A. Burgoyne, and I. Fujinaga. 2007c. “Reducing Costs for Digitizing Early Music with Dynamic Adaptation.” In *Proceedings of the European Conference on Digital Libraries*, 471–74. Budapest, Hungary.

Risk, L., L. Mok, A. Hankinson, and J. Cumming. Forthcoming. “Computational Ranking of Melodic Similarity in French-Canadian Instrumental Dance Tunes.” In *Conference of the International Society for Music Information Retrieval (ISMIR)*.

Roland, P., A. Hankinson, and L. Pugin. 2014. “Early Music and the Music Encoding Initiative.” *Early Music* 42, no. 4: 605–11.

Schubert, P., and J. Cumming. 2015. “Another Lesson from Lassus: Using Computers to Analyze Counterpoint.” *Early Music* 43, no. 4.

Schubert, P., and J. Cumming. 2012. “Text and Motif c. 1500: A New Approach to Text Underlay.” *Early Music* 40, no. 1: 3–14.

Schubert, P. 2006. *Baroque Counterpoint*. Upper Saddle River, NJ: Prentice Hall.

Schubert, P. 1999. *Modal Counterpoint, Renaissance Style*. New York: Oxford University Press.

Sigler, A., J. Wild, and E. Handelman. Forthcoming. “Schematizing the Treatment of Dissonance in 16th-Century Counterpoint.” In *Proceedings of the Conference of the International Society for Music Information Retrieval*.

Siedenburg, K., I. Fujinaga, and S. McAdams. 2014 “Technologies of Timbre: On Audio Features and Evaluation in Interdisciplinary Music Research.” In *Proceedings of the 9th Conference on Interdisciplinary Musicology*. Berlin, Germany.

Smith, D. J., and R. Taylor. *Networks of Music and Culture in the Late Sixteenth and Early Seventeenth Centuries*. Aldershot: Ashgate, 2013.

Thompson, J., C. McKay, J. A. Burgoyne, and I. Fujinaga. 2009. “Additions and Improvements to the ACE 2.0 Music Classifier.” *Proceedings of the International Society for Music Information Retrieval Conference*, 435–40. Kobe, Japan.

Tillmann, B., E. Rusconi, C. Traube, B. Butterworth, C. Umilta, and I. Peretz. 2011. “Fine-Grained Pitch Processing of Music and Speech in Congenital Amusia.” *Journal of the Acoustical Society of America* 130: 4089–96.

Traube, C. 2015. “La notation du timbre instrumental : noter la cause ou l’effet dans le rapport geste-son.” *Circuit : musiques contemporaines* 25: 21–37.

Traube, C. 2013. “De la thermoception à la perception auditive : en quête de l’identité du son froid. L’imaginaire du Nord et du froid en musique : esthétique d’une musique nordique.” *Les Cahiers de la Société québécoise de recherche en musique* 14: 9–15.

Taylor, R. Forthcoming. “Not Lost in Translation: Re-Engaging with Frescobaldi’s Keyboard Music Through Contemporary Works for Lute and Chitarrone by Kapsberger” and “‘Faiseur d’instruments’: The Centrality of Hubert Bédard in the Historical Keyboard Movement.” In *Perspectives on Early Keyboard Music and Revivalism in the Twentieth Century: In Celebration of Kenneth Gilbert*, edited by R. Taylor and H. Knox. Aldershot: Ashgate.

Taylor, R., and H. Knox, ed. Forthcoming. *Perspectives on Early Keyboard Music and Revivalism in the Twentieth Century: In Celebration of Kenneth Gilbert*. Aldershot: Ashgate.

Taylor, R. 2013. “Politics, Religion, Style and the Passamezzo Galliards of Byrd and Philips: A Discussion of Networks Involving Byrd and His Disciples.” In *Networks of Music and Culture in the Late Sixteenth and Early Seventeenth Centuries*, edited by D. J. Smith and R. Taylor, 71–91. Aldershot: Ashgate, 2013.

Taylor, R. 2011. “Le Fonds István Anhalt à Bibliothèque et Archives Canada: Autoconstruction et rôle du lieu dans l’œuvre du compositeur.” In *Centre and Periphery, Roots and Exile: Interpreting the Music of István Anhalt, Gyorgy Kurtag, and Sandor Veress*, edited by F. Sallis, R. Elliott, and K. DeLong, 199–219. Waterloo: Wilfred Laurier University Press, 2011.

Taylor, R. 2002. “Peter Philips, Composer, Priest, and Man of Intrigue.” *Lute: Journal of the Lute Society* 41: 13–32.

Taylor, R. 2000. “Peter Philips (ca. 1560–1628) and the English College, Rome.” In *Proceedings of the Fourth Alamire Congress, The Di Martinelli Music Collection/ Musical Life in Collegiate Churches in the Low Countries and Europe/ Chant & Polyphony*, edited by E. Schreurs and B. Bouckaert, 145–69. Leuven: Alamire Foundation.

Vigliensoni, G., and I. Fujinaga. 2014. “Identifying Time Zones in a Large Dataset of Music Listening Logs.” In *Proceedings of the International Workshop on Social Media Retrieval and Analysis*, 27–32. Gold Coast, Australia.

Vigliensoni, G. J. A. Burgoyne, A. Hankinson, and I. Fujinaga. 2011. “Automatic Pitch Detection in Printed Square Notation.” *Proceedings of the International Society for Music Information Retrieval Conference*, 423–28. Miami, FL.

Vigliensoni, G., C. McKay, and I. Fujinaga. 2010. “Using jWebMiner 2.0 to Improve Music Classification Performance by Combining Different Types of Features Mined from the Web.” In *Proceedings of the International Society for Music Information Retrieval Conference*, 607–12. Utrecht, Netherlands.

Weiss, S. F., and I. Fujinaga. 2010. “New Evidence for the Origin of Kettledrums in Western Europe.”
*Journal of the American Music Instrument Society* 37: 5–18.

Winters, R. M., and J. Cumming. 2014. “Sonification of Symbolic Music in the ELVIS Project.” In *Proceedings of the 20th International Conference on Auditory Display (ICAD–2014)*. New York, NY. hdl.handle.net/1853/52072. -->
